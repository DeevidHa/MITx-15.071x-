---
title: "R Notebook"
output: html_notebook
---
# understanding why people vote
In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is social, or extrinsic, pressure. To quote the first paragraph of their 2008 research paper:

Among the most striking features of a democratic political system is the participation of millions of voters in elections. Why do large numbers of people vote, despite the fact that ... "the casting of a single vote is of no significance where there is a multitude of electors"? One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail to participate in elections. Voters' sense of civic duty has long been a leading explanation of vote turnout...


#The data
The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a "control" group, and the rest were categorized into one of four "treatment" groups. These five groups correspond to five binary variables in the dataset.

1. "Civic Duty" (variable civicduty) group members were sent a letter that simply said "DO YOUR CIVIC DUTY - VOTE!"
2. "Hawthorne Effect" (variable hawthorne) group members were sent a letter that had the "Civic Duty" message plus the additional message "YOU ARE BEING STUDIED" and they were informed that their voting behavior would be examined by means of public records.
3. "Self" (variable self) group members received the "Civic Duty" message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records.
4. "Neighbors" (variable neighbors) group members were given the same message as that for the "Self" group, except the message not only had the household voting records but also that of neighbors - maximizing social pressure.
5. "Control" (variable control) group members were not sent anything, and represented the typical voting situation.
Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).
_______________________________________________________________________________________________________________
###### Problem 1.1 - Exploration and Logistic Regression
We will first get familiar with the data. Load the CSV file gerber.csv into R. What proportion of people in this dataset voted in this election?
```{r}
gerber = read_csv("Downloads/gerber.cv")
str(gerber)

table(gerber$voting)
```
>answer: 108696/(108969+235388) = 0.3158996

###### Problem 1.2 - Exploration and Logistic Regression
Which of the four “treatment groups” had the largest percentage of people who actually voted (voting = 1)?
```{r}
tapply(gerber$voting, gerber$civicduty, mean) 
tapply(gerber$voting, gerber$hawthorne, mean)
tapply(gerber$voting, gerber$self, mean)
tapply(gerber$voting, gerber$neighbors, mean)
```
###### Problem 1.3 - Exploration and Logistic Regression
Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Use all the data to build the model (DO NOT split the data into a training set and testing set). Which of the following coefficients are significant in the logistic regression model? Select all that apply.
```{r}
logmodel <- glm(voting ~ civicduty + hawthorne +
                  self + neighbors,
                  data = gerber, family = binomial)
summary(logmodel)
```
###### Problem 1.4 - Exploration and Logistic Regression
Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don't need to use the newdata argument since we didn't split our data.)

```{r}
predictLog= predict(logmodel, type="response")
table(gerber$voting, predictLog > 0.3)
(134513+51966)/(134513+51966+100875+56730)
```
>accuracy of the logistic regression model = 0.5419578

###### Problem 1.5 - Exploration and Logistic Regression
Using a threshold of 0.5, what is the accuracy of the logistic regression model?
```{r}
table(gerber$voting, predictLog >0.5)
(235388+0)/(108696+235388)
```
>accuracy is  0.6841004

###### Problem 1.6 - Exploration and Logistic Regression
Compare your previous two answers to the percentage of people who did not vote (the baseline accuracy) and compute the AUC of the model. What is happening here?
```{r}
library(ROCR)
table(gerber$voting)
235388/(108969+235388) = 0.6835581 #baseline

ROCRpred=prediction(predictLog, gerber$voting)
as.numeric(performance(ROCRpred, "auc")@y.values)
```
###### Problem 2.1 - Trees
We are interested in building a tree to explore the fraction of people who vote, or the probability of voting.Plot the tree. What happens, and if relevant, why?
```{r}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne +
                    self + neighbors, data = gerber)
prp(CARTmodel)
```
>No variables are used (the tree is only a root node) - none of the variables make a big enough effect to be split on.

###### Problem 2.2 - Trees
to force the complete tree to be built. Then plot the tree. What do you observe about the order of the splits?
Now build the tree using the command:
```{r}
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
```
>We saw in Problem 1 that the highest fraction of voters was in the Neighbors group, followed by the Self group, followed by the Hawthorne group, and lastly the Civic Duty group. And we see here that the tree detects this trend.

###### Problem 2.3 - Trees
Using only the CART tree plot, determine what fraction (a number between 0 and 1) of "Civic Duty" people voted:
#answer: 0.31

###### Problem 2.4 - Trees
Make a new tree that includes the "sex" variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group. In the control group, which gender is more likely to vote?

```{r}
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)

```
>answer : men(0)

###### Problem 3.1 - Interaction Terms
n the "control" only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group? You can use the absolute value function to get answer, i.e. abs(Control Prediction - Non-Control Prediction). Add the argument "digits = 6" to the prp command to get a more accurate estimate.
```{r}
CARTcontrol= rpart(voting ~ control, data=gerber, cp=0.0)
prp(CARTcontrol,digits = 6)
 abs(0.296638-0.34)  #0.043362
 
CARTsex= rpart(voting ~ control+ sex, data=gerber, cp=0.0)
prp(CARTsex,digits = 6)
```
###### Problem 3.3 - Interaction Terms
Going back to logistic regression now, create a model using "sex" and "control". Interpret the coefficient for "sex":

```{r}
logmodel = glm(voting ~ control + sex, data = gerber, family = binomial)
summary(logmodel)
```
>If you look at the summary of the model, you can see that the coefficient for the "sex" variable is -0.055791. This means that women are less likely to vote, since women have a larger value in the sex variable, and a negative coefficient means that larger values are predictive of 0.

###### Problem 3.4 - Interaction Terms
The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ). What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case? Give an answer with five numbers after the decimal point.

```{r}
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(LogModelSex, newdata=Possibilities, type="response")
abs(0.2908065-0.290456)


```
>The CART tree predicts 0.290456 for the (Woman, Control) case, and the logistic regression model predicts 0.2908065. So the absolute difference, to five decimal places, is 0.00035.
Problem 3.5 - Interaction Terms

###### Problem 3.5 - Interaction Terms

So the difference is not too big for this dataset, but it is there. We're going to add a new term to our logistic regression now, that is the combination of the "sex" and "control" variables - so if this new variable is 1, that means the person is a woman AND in the control group. 
```{r}
logmodel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
```
>This coefficient is negative, so that means that a value of 1 in this variable decreases the chance of voting. This variable will have variable 1 if the person is a woman and in the control group.

###### Problem 3.6 - Interaction Terms
Run the same code as before to calculate the average for each group:
predict(LogModel2, newdata=Possibilities, type="response")

Now what is the difference between the logistic regression model and the CART model for the (Woman, Control) case? Again, give your answer with five numbers after the decimal point.

```{r}
predict(logmodel2, newdata=Possibilities, type="response")
```

###### Problem 3.7 - Interaction Terms
This example has shown that trees can capture nonlinear relationships that logistic regression can not, but that we can get around this sometimes by using variables that are the combination of two variables. Should we always include all possible interaction terms of the independent variables when building a logistic regression model?
> No, We should not use all possible interaction terms in a logistic regression model due to overfitting. Even in this simple problem, we have four treatment groups and two values for sex. If we have an interaction term for every treatment variable with sex, we will double the number of variables. In smaller data sets, this could quickly lead to overfitting.

_______________________________________________________________________________________________________________

# Text analytics

One of the earliest applications of the predictive analytics methods we have studied so far in this class was to automatically recognize letters, which post office machines use to sort mail. In this problem, we will build a model that uses statistics of images of four letters in the Roman alphabet -- A, B, P, and R -- to predict which letter a particular image corresponds to.

Note that this is a multiclass classification problem. We have mostly focused on binary classification problems (e.g., predicting whether an individual voted or not, whether the Supreme Court will affirm or reverse a case, whether or not a person is at risk for a certain disease, etc.). In this problem, we have more than two classifications that are possible for each observation, like in the D2Hawkeye lecture. 

The file letters_ABPR.csv contains 3116 observations, each of which corresponds to a certain image of one of the four letters A, B, P and R. The images came from 20 different fonts, which were then randomly distorted to produce the final images; each such distorted image is represented as a collection of pixels, each of which is "on" or "off". For each such distorted image, we have available certain statistics of the image in terms of these pixels, as well as which of the four letters the image is. This data comes from the UCI Machine Learning Repository.

This dataset contains the following 17 variables:

*letter = the letter that the image corresponds to (A, B, P or R)
*xbox = the horizontal position of where the smallest box covering the letter shape begins.
*ybox = the vertical position of where the smallest box covering the letter shape begins.
*width = the width of this smallest box.
*height = the height of this smallest box.
*onpix = the total number of "on" pixels in the character image
*xbar = the mean horizontal position of all of the "on" pixels
*ybar = the mean vertical position of all of the "on" pixels
*x2bar = the mean squared horizontal position of all of the "on" pixels in the image
*y2bar = the mean squared vertical position of all of the "on" pixels in the image
*xybar = the mean of the product of the horizontal and vertical position of all of the "on" pixels in the image
*x2ybar = the mean of the product of the squared horizontal position and the vertical position of all of the    "on" pixels
*xy2bar = the mean of the product of the horizontal position and the squared vertical position of all of the "on" pixels
*xedge = the mean number of edges (the number of times an "off" pixel is followed by an "on" pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image
*xedgeycor = the mean of the product of the number of horizontal edges at each vertical position and the vertical position
*yedge = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image
*yedgexcor = the mean of the product of the number of vertical edges at each horizontal position and the  horizontal position

######Problem 1.1 - Predicting B or not B

create a new variable isB in the dataframe, which takes the value "TRUE" if the observation corresponds to the letter B, and "FALSE" if it does not. You can do this by typing the following command into your R console:
```{r}
library(caTools)
letters_ABPR <- read_csv("Downloads/letters_ABPR.csv")
letters = letters_ABPR
letters$isB = as.factor(letters$letter == "B")
```
Now split the data set into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. The first argument to sample.split should be the dependent variable "letters$isB". Remember that TRUE values from sample.split should go in the training set.
```{r}
set.seed(1000)# to get the same split everytime
spl = sample.split(letters$isB, SplitRatio = 0.5)
train = subset(letters, spl == TRUE)
test = subset(letters, spl== FALSE)
```
Before building models, let's consider a baseline method that always predicts the most frequent outcome, which is "not B". What is the accuracy of this baseline method on the test set?
```{r}
table(train$isB)
1175/(1175+383)
```
>1175/(1175+383) = 0.754172

######Problem 1.2 - Predicting B or not B
Now build a classification tree to predict whether a letter is a B or not, using the training set to build your model. Remember to remove the variable "letter" out of the model, as this is related to what we are trying to predict! To just remove one variable, you can either write out the other variables, or remember what we did in the Billboards problem in Week 3, and use the following notation:

CARTb = rpart(isB ~ . - letter, data=train, method="class")

We are just using the default parameters in our CART model, so we don't need to add the minbucket or cp arguments at all. We also added the argument method="class" since this is a classification problem.

What is the accuracy of the CART model on the test set? (Use type="class" when making predictions on the test set.)

```{r}
CARTb=rpart(isB ~ . - letter, data = train, method = "class")
predictions = predict(cartB, newdata = test, type = "class")
cmat_CART<-table(test$isB, predictions) 
cmart_CART
```
> (1118+340)/(1118+57+43+340)
thus, the overall accuracy of the classifcation tree model is 0.93158151

######PROBLEM 1.3 - PREDICTING B OR NOT B   

build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. You should use all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). Use the default settings for ntree and nodesize (don't include these arguments at all). Right before building the model, set the seed to 1000. (NOTE: You might get a slightly different answer on this problem, even if you set the random seed. This has to do with your operating system and the implementation of the random forest algorithm.)

```{r}
library(randomForest)
set.seed(1000)
RandomforestB = randomForest(isB ~ xbox + ybox + width + 
                               height + onpix + xbar + 
                               ybar + x2bar + y2bar + 
                               xybar + x2ybar + xy2bar + 
                               xedge + xedgeycor + yedge + 
                               yedgexcor, data=train)
predictionsB  = predict(RandomforestB, newdata=test)
forest = table(test$isB, predictionsB)
forest
```
>(1166+371)/(1166+371+12+9)


######Problem 2.1 - Predicting the letters A, B, P, R

Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.

As we saw in the D2Hawkeye lecture, building a multiclass classification CART model in R is no harder than building the models for binary classification problems. Fortunately, building a random forest model is just as easy.

The variable in our data frame which we will be trying to predict is "letter". Start by converting letter in the original data set (letters) to a factor by running the following command in R:

letters$letter = as.factor( letters$letter )

Now, generate new training and testing sets of the letters data frame using letters$letter as the first input to the sample.split function. Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)

In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.

What is the baseline accuracy on the testing set?

```{r}
library(caTools)
set.seed(2000)
spl == sample.split(letters$letter, splitratio = 0.5)
train2 == subset(letters, spl == TRUE)
test2 == subset(letters, spl == FALSE)

table(train2$letter)

RandomforestABPR=randomForest(letter ~ xbox + ybox + width + 
                                height + onpix + xbar +
                                ybar + x2bar + y2bar + 
                                xybar + x2ybar + xy2bar + 
                                xedge + xedgeycor + yedge + 
                                yedgexcor, data=train2)

predictionsABPR = predict(RandomforestABPR, newdata=test2)
forestABPR = table(test2$letter, predictionsABPR)
forestABPR
(test2$letter)
```
>401/nrow(test2) = 0.2573813

######Problem 2.2 - Predicting the letters A, B, P, R
```{r}
CARTletter= rpart(letter ~ .-isB, data = train2,method="class" ) 
prp(CARTletter)

predictLetter= predict(CARTletter, newdata = test2, type = "class")
cmat_CART<-table(test2$letter, predictLetter)
cmat_CART
```
> 348+318+363+340)/nrow(test2) = 0.8786906


###### Problem 2.3 - Predicting the letters A, B, P, R
```{r}
set.seed(1000)

# Build random forest model
RFletter= randomForest(letter ~ .-isB, data = train2) 

#What is the test set accuracy of your random forest model?
predictLetter= predict(RFletter, newdata=test2)

#Now lets assess the accuracy of the model through confusion matrix
cmat_forest<-table(test2$letter, predictLetter)  #first arg is thr true outcomes and the second is the predicted outcomes
cmat_forest
accu_forest <- (cmat_forest[1,1] + cmat_forest[2,2] + cmat_forest[3,3] + cmat_forest[4,4])/sum(cmat_forest)
accu_forest  
```
>(390+380 +393+364)/nrow(test2) = 0.9801027 


_______________________________________________________________________________________________________________
#Predicting Earnings from Census Data

####### problem 1.1 logistic regresssion model
build a logistic regression model to predict the dependent variable "over50k", using all of the other variables in the dataset as independent variables. Use the training set to build the model.

Which variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.) Select all that apply.
```{r}
library(caTools)
Census = read.csv("Downloads/census.csv")
str(Census)

set.seed(2000)
spl<-sample.split(Census$over50k, SplitRatio=0.6)#the DV is split into 60% training set and 40% test set
train<-subset(Census,spl== TRUE)
test <-subset(Census, spl== FALSE)


model50K = glm(over50k ~ age + workclass + education +
                 maritalstatus + occupation + relationship +
                 race + sex + capitalgain + capitalloss +
                 hoursperweek + nativecountry, data = train, family = "binomial", )

summary(model50K)
```
> Answer: age, workclass, education, maritalstatus, occupation, relationship, sex, capitalgain, capitalllos, hoursperweek

###### Problem 1.2 - A Logistic Regression Model

What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)
```{r}
#Out-of-Sample predictions of the Logistic Regression model
predictTest = predict(model50k, newdata=test, type="response")

#Accuracy of model using threshold of 0.5
accuracy = table(test$over50k, predictTest > 0.5)
accuracy
#sum up false negatives + true postivies divided by total amount.
```
> (9051+1888)/(9051+662+1190+1888) = 0.8552107


######Problem 1.3 - A Logistic Regression Model
What is the baseline accuracy for the testing set?
```{r}
table(test$over50k) 
```
> 9713/(9713+3078) = 0.7593621

###### PROBLEM 1.4 - A LOGISTIC REGRESSION MODEL  
```{r}
# evaluate our model using the ROC curve with rocr package
library(ROCR)

predROCR = prediction(predictTest, test$over50k)
predauc<- performance(predROCR, "tpr", "fpr")
plot(predauc)
as.numeric(performance(ROCRpred, "auc")@y.values)

```

Problem 2.1 - A CART Model
We have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.

Let us now build a classification tree to predict "over50k". Use the training set to build the model, and all of the other variables as independent variables. Use the default parameters, so don't set a value for minbucket or cp. Remember to specify method="class" as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.

```{r}
cart50k=rpart(over50k ~age + workclass + education +
                 maritalstatus + occupation + relationship +
                 race + sex + capitalgain + capitalloss +
                 hoursperweek + nativecountry, data = train, method = "class")
prp(cart50k)
```
> 4 splits in total.

######Problem 2.4 - A CART Model

What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type="class", or generate probabilities and use a threshold of 0.5 like in logistic regression.)

This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.
```{r}
pred = predict(cart50k, newdata = test, type = "class") #We need to give type = "class" if we want the majority class predictions.This is like using a threshold of 0.5.

#Now lets assess the accuracy of the model through confusion matrix
over50kcart<-table(test$over50k, pred)  #first arg is the true outcomes and the second is the predicted outcomes
over50kcart

#compute the accuracy by summing up false negativces and true positives divided by the total
```
>#(9243+1596)/(9243+470+1482+1596) = 0.8473927

####### Problem 2.6 - A CART model

```{r}
PredictROC = predict(model50k, newdata = test)


pred = prediction(PredictROC[,2], test$over50k)
perf = performance(pred, "tpr", "fpr")

#plot
plot(perf)

#AUC of model
as.numeric(performance(pred, "auc")@y.values)

```

###### Problem 3.1 - A Random Forest Model
et us now build a random forest model to predict "over50k", using the dataset "trainSmall" as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest "can not handle categorical predictors with more than 32 categories", re-build the model without the nativecountry variable as one of the independent variables.)

```{r}
set.seed(1)
trainSmall = train[sample(nrow(train),2000), ]
library(randomForest)

CensusForest = randomForest(over50k ~ ., data = trainsmall)

#make predictions with forest model
predictForest = predict(CensusForest, newdata=test)
#Assess the accuracy of the model through confusion matrix
cmat_forest = table(test$over50k, predictForest)
cmat_forest
```
> Accuracy of forest is = (8957 + 1940)/(8957+756+1138+1940)


##### problem 3.2

As we discussed in lecture, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important.

One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of R code (replace "MODEL" with the name of your random forest model):
```{r}
vu = varUsed(RandomForest, count=TRUE)

vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)

dotchart(vusorted$x, names(RandomForest$forest$xlevels[vusorted$ix]))
```
This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis). Which of the following variables is the most important in terms of the number of splits?
> Age

###### problem 3.3 

A different metric we can look at is related to "impurity", which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. To compute this metric, run the following command in R (replace "MODEL" with the name of your random forest model):
```{r}
varImpPlot(CensusForest)
```
>occupation


###### problem 4.1


We now conclude our study of this data set by looking at how CART behaves with different choices of its parameters.

Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments, by using the following command:

cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))

Also, remember to use the entire training set "train" when building this model. The train function might take some time to run.

Which value of cp does the train function recommend?

```{r}

library(caret)
library(e1071)

set.seed(2)

#Define cross-validation experiment

#specify that we are going to use k-fold cross validation with 10 folds:
numFolds = trainControl( method = "cv", number = 10 )

#Specify the grid of cp values that we wish to evaluate:
cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))
#This will define our cp parameters to test as numbers from 0.002 to 0.1, in increments of 0.002.

#Perform the cross validation by running the train function and view the result:
save_CV<-train(over50k~.,data=train, method="rpart",trControl=numFolds,   tuneGrid=cartGrid)
save_CV
plot(save_CV)
plot(save_CV$results$cp, save_CV$results$Accuracy, type="l", xlab="cp", ylab="accuracy")


```


###### problem 4.2 
```{r}
model = rpart(over50k~., data=train, method="class", cp=0.002)


predictTest = predict(model, newdata=test, type="class")


table(test$over50k, predictTest)
```
> The accuracy is (9178+1838)/(9178+535+1240+1838) = 0.8612306.
