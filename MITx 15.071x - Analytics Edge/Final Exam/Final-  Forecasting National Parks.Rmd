---
title: "R Notebook"
output: html_notebook
---
# FORECASTING NATIONAL PARKS VISITS

The U.S. National Parks System includes 417 areas including national parks, monuments, battlefields, military parks, historical parks, historical sites, lakeshores, seashores, recreation areas, scenic rivers and trails, and the White House (see map in Figure 1). Every year, hundreds of millions of recreational visitors come to the parks. What do we know about the parks that can affect the visitor counts? Can we forecast the monthly visits to a given park accurately? To derive insights and answer these questions, we take a look at the historical visits data and the parks information released by the National Parks Service (NPS). 

For this problem, we obtained monthly visits data between 2010 and 2016 (source: https://irma.nps.gov/Stats/Reports/National). We also got park-specific data via the NPS API (https://www.nps.gov/subjects/developer/get-started.htm). The aggregated dataset park_visits.csv results in a total of 12 variables and 25587 observations. Each observation contains one record per park per month. Here's a detailed description of the variables:

- **ParkName**: The full name of the park.

- **ParkType**: The type of the park. For this study we restrict ourselves to the following more frequently visited types: National Battlefield, National Historic Site, National Historical Park, National Memorial, National Monument, National Park, National Recreation Area, and National Seashore.

- **Region**: The region of the park, including Alaska, Intermountain, Midwest, National Capital, Northeast, Pacific West, and Southeast.

- **State**: The abbreviation of the state where the park resides.

- **Year, Month**: the year and the month for the visits.

- **lat, long**: Latitude and longitude of the park.

- **Cost**: a simple extraction of the park's entrance fee. Some parks may have multiple levels of entrance fees (differ by transportation methods, age, military status, etc.); for this problem, we only extracted the first available cost information.

- **logVisits**: Natural logarithm of the recreational visits (with one added to the visits to avoid taking logs of zero) to the park in the given year and month.

- **laglogVisits**: the logVisits from last month.

- **laglogVisitsYear**: the logVisits from last year.

________________________________________________________________________________________________________________

##### Problem 1 - Number of National Parks in Jan 2016
Load park_visits.csv into a data frame called visits.
Let's first look at the visits in July 2016. Subset the observations to this year and month, name it visits2016jul. Work with this data subset for the next three problems.
1. *Which park type has the most number of parks?*
2. *Which specific park has the most number of visitors?*
```{r}
data = read_csv("Downloads/asset-v1_MITx+15.071x+2T2017+type@asset+block@park_visits.csv")
str(data)
table(data)
visits2016jul = subset(data, Year == 2016 & Month == 7)
visits2016jul[which.max(visits2016jul$logVisits),]
```
> Parktype : National Historic Site 
  Parkname: Great Smoky Mountains
  
##### Problem 2 - Relationship Between Region and Visits
Which region has the highest average log visits in July 2016?
What is the average log visits for the region in July 2016 with:
1. *the highest average log visits?*
2. *the lowest average log visits?*
```{r}
sort(tapply(visits2016jul$logVisits, visits2016jul$Region, mean))
```
> Region: Pacific West 
  Highest avg: 10.767849 
  Lowest avg:  9.374157
  
##### Problem 3 - Relationship Between Cost and Visits
1. *What is the correlation between entrance fee (the variable cost) and the log visits in July 2016?*
2. *Choose the most reasonable possible answer from the following statements:*
```{r}
cor(visits2016jul$cost, visits2016jul$logVisits)
```
> Correlation between entrance fee(the variable cost) and the log visits in july 2016 is: 0.4010611 &
  Higher entrance fees are associated with higher log visits, likely because more expensive parks are often more   popular due to other features of the parks

##### Problem 4 - Time Series Plot of Visits
Let's now look at the time dimension of the data. Subset the original data (visits) to "Yellowstone NP" only and save as ys. Use the following code to plot the logVisits through the months between 2010 and 2016:
**ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)**
**plot(ys_ts)**
*What observations do you make?*
```{r}
ys = subset(data, data$ParkName=="Yellowstone NP")
ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)
plot(ys_ts)
```
> Between the years, the shapes are largely similar & The log visits are highly cyclical, with the peaks in the summer time.

##### Problem 5 - Missing Values
Note that there are some NA's in the data - you can run colSums(is.na(visits)) to see the summary.
Why do we have NA's in the laglogVisits and laglogVisitsYear? These variables were created by lagging the log visits by a month or by a year.
To deal with the missing values, we will simply remove the observations with the missing values first (there are more sophisticated ways to work with missing values, but for this purpose removing the observations is fine). Run the following:
**visits = visits[rowSums(is.na(visits)) == 0, ]**
How many observations are there in visits now?

```{r}
data = data[rowSums(is.na(data))==0,]
colSums(is.na(data))
str(data)
```
> 
These are lagged variables and the earlier data is not available for the first months.
21855 observations

##### Problem 6 - Predicting Visits
We are interested in predicting the log visits. Before doing the split, let's also make Month a factor variable by including the following:
**visits$Month = as.factor(visits$Month)**
Subset our dataset into a training and a testing set by splitting based on the year: training would contain 2010-2014 years of data, and testing would be 2015-2016 data.
Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
1. *What's the coefficient of the laglogVisits variable?*
2. *What's the out-of-sample R2 in the testing set for this simple model?*
```{r}
libary(caTools)
data$Month = as.factor(data$Month)
Train = subset(data, data$Year<=2014)
Test = subset(data,data$Year>=2015)
mod = lm(logVisits ~ laglogVisits, data = Train)
logVisitsPred = predict(mod, newdata = Test)
TSS = sum((Test$logVisits-mean(Train$logVisits))**2)
SSE = sum((logVisitsPred-Test$logVisits)**2)
1-SSE/TSS
```
> laglogVisits coefficient summary(mod): 0.927945, 
  out-of-sample R2 : 0.8975923

##### Problem 7 - Add New Variables
We see that the model achieves good predictive power already simply using the previous month's visits. To see if the other knowledge we have about the parks can improve the model, let's add these variables in a new model.
The new model would have the following variables:
laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost
Looking at the model summary, which of the following statements are correct (significance at 0.05 level)?
```{r}
model = lm(logVisits ~ laglogVisits + laglogVisitsYear + Year +
             Month + Region + ParkType + cost, data = Train)
summary(model)
```
> 1.Both the log visits from last month and last year are significant and are positively associated with the       current log visits.
  2.None of the park types are significant from the baseline park type (National Battlefield).
  
##### Problem 8 - Out-of-Sample R2
In the new model, what's the out-of-sample R2 in the testing set?
```{r}
logVisitsPred2=predict(model, newdata=Test)
MSE2 <-sum((Test$logVisits-logVisitsPred2)**2)
1-MSE2/TSS
```
> 0.937253

##### Problem 10 - Regression Trees with CV
The out-of-sample R2 does not appear to be very good under regression trees, compared to a linear regression model. We could potentially improve it via cross validation.
1. *Set seed to 201, run a 10-fold cross-validated cart model, with cp ranging from 0.0001 to 0.005 in             increments of 0.0001. What is optimal cp value on this grid?*
2.  *Looking at the validation R2 versus the cp value, we can further refine the cp range. In what direction        should it change?*
```{r}
cpGrid = expand.grid(.cp=seq(0.0001,0.005,0.0001))
cpControl = trainControl(method = "cv", number = 10)
set.seed(201)
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = Train, method = "rpart", trControl = cpControl, tuneGrid = cpGrid)
```
> 0.0001,
  smaller values of cp

##### Problem 11 - Final Regression Tree
Rerun the regression tree on the training data, now using the cp value equal to the one selected in the previous problem (under the original range). Note: do not get the tree from the cross-validation directly.
*What is the out-of-sample R2 in the testing set?*
```{r}
RegressionTree = rpart(logVisits ~ laglogVisits+laglogVisitsYear+ 
                           Year+Month+ 
                           Region+ParkType+
                           cost, data=Train, control=rpart.control(cp=0.0001))
treePred = predict(RegressionTree, newdata = Test)
MSE3 = sum((treePred-Test$logVisits)**2)
1-MSE3/TSS
```
> 0.937113

##### Problem 12 - Random Forest
We can potentially further improve the models by using a random forest. Set seed to 201 again. Train a random forest model with the same set of covariates, and using just default parameters (no need to specify). This may take a few minutes.
*What is the R2 on the testing set for the random forest model?*
```{r}
library(randomForest)
set.seed(201)
RF = randomForest(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = Train)
RFPred = predict(RF, newdata = Test)
MSE4 = sum((RFPred-Test$logVisits)**2)
1-MSE4/TSS
```
> 0.9472