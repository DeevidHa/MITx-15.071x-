---
title: "unit 5 - part 2"
output: html_notebook
---
early every email user has at some point encountered a "spam" email, which is an unsolicited message often advertising a product, containing links to malware, or attempting to scam the recipient. Roughly 80-90% of more than 100 billion emails sent each day are spam emails, most being sent from botnets of malware-infected computers. The remainder of emails are called "ham" emails.

As a result of the huge number of spam emails being sent across the Internet each day, most email providers offer a spam filter that automatically flags likely spam messages and separates them from the ham. Though these filters use a number of techniques (e.g. looking up the sender in a so-called "Blackhole List" that contains IP addresses of likely spammers), most rely heavily on the analysis of the contents of an email via text analytics.

In this homework problem, we will build and evaluate a spam filter using a publicly available dataset first described in the 2006 conference paper "Spam Filtering with Naive Bayes -- Which Naive Bayes?" by V. Metsis, I. Androutsopoulos, and G. Paliouras. The "ham" messages in this dataset come from the inbox of former Enron Managing Director for Research Vincent Kaminski, one of the inboxes in the Enron Corpus. One source of spam messages in this dataset is the SpamAssassin corpus, which contains hand-labeled spam messages contributed by Internet users. The remaining spam was collected by Project Honey Pot, a project that collects spam messages and identifies spammers by publishing email address that humans would know not to contact but that bots might target with spam. The full dataset we will use was constructed as roughly a 75/25 mix of the ham and spam messages.

The dataset contains just two fields:

text: The text of the email.
spam: A binary variable indicating if the email was spam.
 

IMPORTANT NOTE: This problem (Separating Spam from Ham) continues on the next page with additional exercises. The second page is optional, but if you want to try it out, remember to save your work so you can start the next page where you left off here.

________________________________________________________________________________________________________________

###### problem 1.1 - Loading the dataset

Begin by loading the dataset emails.csv into a data frame called emails. Remember to pass the stringsAsFactors=FALSE option when loading the data.

How many emails are in the dataset?
```{r}
emails = read.csv("Downloads/emails.csv", stringsAsFactors = FALSE)
str(emails)
summary(emails)
```
> summary(emails) : text length : 5728

###### problem 1.2 - Loading the dataset

How many of the emails are spam?
```{r}
table(emails$spam)
```
> 1368 spam

###### problem 1.3 - Loading the dataset

Which word appears at the beginning of every email in the dataset? Respond as a lower-case word with punctuation removed.
```{r}
emails$text[1]
```

###### problem 1.4 - Loading the dataset
Could a spam classifier potentially benefit from including the frequency of the word that appears in every email?
> Yes -- the number of times the word appears might help us differentiate spam from ham.
explanation : We know that each email has the word "subject" appear at least once, but the frequency with which it appears might help us differentiate spam from ham. For instance, a long email chain would have the word "subject" appear a number of times, and this higher frequency might be indicative of a ham message.

###### problem 1.5 - Loading the dataset
The nchar() function counts the number of characters in a piece of text. How many characters are in the longest email in the dataset (where longest is measured in terms of the maximum number of characters)?
```{r}
max(nchar(emails$text))
```
> 43952

###### problem 1.6 - Loading the dataset
Which row contains the shortest email in the dataset? (Just like in the previous problem, shortest is measured in terms of the fewest number of characters.)
```{r}
which.min(nchar(emails$text))
```

###### problem 2.1 - Preparing the corpus
Follow the standard steps to build and pre-process the corpus:
How many terms are in dtm?
```{r}
library(tm) # import text mining package
#1) Build a new corpus variable called corpus.
corpus = VCorpus(VectorSource(emails$text)) 
corpus[[1]]$content # check content
#pre-process data
#2) Using tm_map, convert the text to lowercase.
corpus = tm_map(corpus, content_transformer(tolower)) 
#3) Using tm_map, remove all punctuation from the corpus.
corpus = tm_map(corpus, removePunctuation) 
#4) Using tm_map, remove all English stopwords from the corpus.
corpus = tm_map(corpus, removeWords, stopwords("english")) 
#5) Using tm_map, stem the words in the corpus.
corpus = tm_map(corpus, stemDocument)
#6) Build a document term matrix from the corpus, called dtm.
dtm = DocumentTermMatrix(corpus)
```
> terms:28687

###### problem 2.2 - preparing the corpus
To obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents, and store this result as spdtm (don't overwrite dtm, because we will use it in a later step of this homework). How many terms are in spdtm?
```{r}
spdtm = removeSparseTerms(dtm, 0.95)
spdtm
```
terms : 330

##### Problem 2.3 - Preparing the Corpus
Build a data frame called emailsSparse from spdtm, and use the make.names function to make the variable names of emailsSparse valid.
colSums() is an R function that returns the sum of values for each variable in our data frame. Our data frame contains the number of times each word stem (columns) appeared in each email (rows).  What is the word stem that shows up most frequently across all the emails in the dataset? 
```{r}
#convert the sparse matrix into a data frame that we will be able to use for our predictive models.
emailsSparse <- as.data.frame(as.matrix(spdtm)) 
#To make all variable names R-friendly use:
colnames(emailsSparse)<- make.names(colnames(emailsSparse))
#word stem that shows up the most frequent across the dataset.
which.max(colSums((emailsSparse)))
```
> enron : 92

###### 2.4 - Preparing the Corpus
Add a variable called "spam" to emailsSparse containing the email spam labels. You can do this by copying over the "spam" variable from the original data frame (remember how we did this in the Twitter lecture).

How many word stems appear at least 5000 times in the ham emails in the dataset? Hint: in this and the next question, remember not to count the dependent variable we just added.
```{r}
emailsSparse$spam = emails$spam
sum(colSums(subset(emailsSparse, emailsSparse$spam==0)) >= 5000)
```
> 6

###### Problem 2.5 - Preparing the Corpus
How many word stems appear at least 1000 times in the spam emails in the dataset?
>We can limit the dataset to the spam emails with subset(emailsSparse, spam == 1). Therefore, we can read the most frequent terms with sort(colSums(subset(emailsSparse, spam == 1))). "subject", "will", and "compani" are the three stems that appear at least 1000 times. Note that the variable "spam" is the dependent variable and is not the frequency of a word stem.

###### Problem 2.6 - Preparing the Corpus
The lists of most common words are significantly different between the spam and ham emails. What does this likely imply?
>The frequencies of these most common words are likely to help differentiate between spam and ham.

###### Problem 2.7 - Preparing the Corpus
Several of the most common word stems from the ham documents, such as "enron", "hou" (short for Houston), "vinc" (the word stem of "Vince") and "kaminski", are likely specific to Vincent Kaminski's inbox. What does this mean about the applicability of the text analytics models we will train for the spam filtering problem?
>The models we build are personalized, and would need to be further tested before being used as a spam filter for another person.

###### Problem 3.1 - Building machine learning models

First, convert the dependent variable to a factor with "emailsSparse$spam = as.factor(emailsSparse$spam)".
```{r}
emailsSparse$spam = as.factor(emailsSparse$spam)
```
Next, set the random seed to 123 and use the sample.split function to split emailsSparse 70/30 into a training set called "train" and a testing set called "test". Make sure to perform this step on emailsSparse instead of emails.
```{r}
library(caTools)
set.seed(123)
spl = sample.split(emailsSparse$spam, SplitRatio=0.7)
train = subset(emailsSparse, spl == TRUE)
test = subset(emailsSparse, spl == FALSE)
```
Using the training set, train the following three machine learning models. The models should predict the dependent variable "spam", using all other available variables as independent variables. 
1) A logistic regression model called spamLog. You may see a warning message here - we'll discuss this more later.
```{r}
LOGmodel = glm(spam~., train, family=binomial)
LOGmodel
```
2) A CART model called spamCART, using the default parameters to train the model (don't worry about adding minbucket or cp). Remember to add the argument method="class" since this is a binary classification problem.
```{r}
spamCART = rpart(spam~., train, method = "class")
prp(spamCART)
```
3) A random forest model called spamRF, using the default parameters to train the model, set the random seed to 123 training the model so we all obtain the same results. 
```{r}
set.seed(123)
spamRF = randomForest(spam~., train)
```
For each model, obtain the predicted spam probabilities for the training set. Be careful to obtain probabilities instead of predicted classes, because we will be using these values to compute training set AUC values. Recall that you can obtain probabilities for CART models by not passing any type parameter to the predict() function, and you can obtain probabilities from a random forest by adding the argument type="prob". For CART and random forest, you need to select the second column of the output of the predict() function, corresponding to the probability of a message being spam.
```{r}
predLOG = predict(LOGmodel, type="response")
predCART = predict(spamCART)[,2]
predRF = predict(spamRF, type = "prob")[,2]
```
How many of the training set predicted probabilities from spamLog are less than 0.00001?
```{r}
table(predLOG < 0.00001)
```
>3064

How many of the training set predicted probabilities from spamLog are more than 0.99999?
```{r}
table(predLOG > 0.99999)
```
> 954

How many of the training set predicted probabilities from spamLog are between 0.00001 and 0.99999?
```{r}
table(predLOG >= 0.00001 & predLOG<= 0.99999)
```
> 10

###### Problem 3.2 - Building Machine Learning Models
How many variables are labeled as significant (at the p=0.05 level) in the logistic regression summary output?
```{r}
summary(LOGmodel)
```
>0

##### Problem 3.3 - Building Machine Learning Models
How many of the word stems "enron", "hou", "vinc", and "kaminski" appear in the CART tree? 
```{r}
prp(spamCART)
```
>2

###### Problem 3.4 - Building Machine Learning Models
What is the training set accuracy of spamLog, using a threshold of 0.5 for predictions?

```{r}
#We are interested in the overall accuracy of our model
#First we compute the confusion matrix
cmat_log = table(train$spam, predLOG > 0.5)
cmat_log
```
> Count the true negatives and true positive (3052+954)/(3052+0+4+952) = 0.999501

###### Problem 3.5 - Building Machine Learning Models
What is the training set AUC of spamLog?
```{r}
predictionTrainLog = prediction(predLOG, train$spam)
perf = performance(predictionTrainLog, "tpr", "fpr")
as.numeric(performance(predictionTrainLog, "auc")@y.values)
```
> 0.9999959

###### Problem 3.6 - Building Machine Learning Models
What is the training set accuracy of spamCART, using a threshold of 0.5 for predictions? (Remember that if you used the type="class" argument when making predictions, you automatically used a threshold of 0.5. If you did not add in the type argument to the predict function, the probabilities are in the second column of the predict output.)
```{r}
#We are interested in the overall accuracy of our model
#First we compute the confusion matrix
cmat_CART<-table(train$spam, predCART > 0.5)
cmat_CART
```
>(2885+894)/(2885+167+64+894) = 0.942394

###### Problem 3.7 - Building Machine Learning Models
What is the training set AUC of spamCART? (Remember that you have to pass the prediction function predicted probabilities, so don't include the type argument when making predictions for your CART model.)
```{r}
predictionTrainCart = prediction(predCART, train$spam)
pred = performance(predictionTrainCart, "tpr", "fpr")
as.numeric(performance(predictionTrainCart, "auc")@y.values)
plot(perf, colorize=TRUE) 
```
>0.9696044

###### Problem 3.8 - Building Machine Learning Models

What is the training set accuracy of spamRF, using a threshold of 0.5 for predictions? (Remember that your answer might not match ours exactly, due to random behavior in the random forest algorithm on different operating systems.)
```{r}
cmat_RF=table(train$spam, predRF > 0.5)
cmat_RF
```
>(3015+916)/(3015+37+42+916)= 0.9802993

###### Problem 3.9 - Building Machine Learning Models
What is the training set AUC of spamRF? (Remember to pass the argument type="prob" to the predict function to get predicted probabilities for a random forest model. The probabilities will be the second column of the output.)
```{r}
predictionTrainRF = prediction(predRF, train$spam)
pred = performance(predictionTrainRF, "tpr", "fpr")
as.numeric(performance(predictionTrainRF, "auc")@y.values)
plot(pred, colorize=TRUE) 
```
>0.9978155

###### Problem 3.10 - Building Machine Learning Models
Which model had the best training set performance, in terms of accuracy and AUC?
> Logistic Regression, In terms of both accuracy and AUC, logistic regression is nearly perfect and outperforms the other two models.

###### Problem 4.1 - Evaluating on the Test Set
Obtain predicted probabilities for the testing set for each of the models, again ensuring that probabilities instead of classes are obtained. What is the testing set accuracy of spamLog, using a threshold of 0.5 for predictions?
```{r}
predTestLog=predict(LOGmodel, newdata=test, type="response")
predTestCART=predict(spamCART, newdata=test)[,2]
predTestRF=predict(spamRF, newdata=test, type="prob")[,2]

cmat_log<-table(test$spam, predTestLog> 0.5)
cmat_log
```
>(1257+376)/(1257+376+51+34) = 0.9505239

###### Problem 4.2 - Evaluating on the Test Set

What is the testing set AUC of spamLog?
```{r}
predictionTestLog = prediction(predTestLog,test$spam)
perf = performance(predictionTestLog, "tpr", "fpr")
as.numeric(performance(predictionTestLog, "auc")@y.values)
plot(perf, colorize=TRUE) 
```
> 0.9627517

###### Problem 4.3 - Evaluating on the Test Set
What is the testing set accuracy of spamCART, using a threshold of 0.5 for predictions?
```{r}
cmat_cart = table(test$spam, predTestCART > 0.5)
cmat_cart
```
> (1228+386)/(1228+80+24+386)=0.9394645

###### Problem 4.4 - Evaluating on the Test Set
What is the testing set AUC of spamCART?
```{r}
predictionTestCART = prediction(predTestCART,test$spam)
perf = performance(predictionTestCART, "tpr", "fpr")
as.numeric(performance(predictionTestCART, "auc")@y.values)
```
> 0.963176

###### Problem 4.5 - Evaluating on the Test Set
What is the testing set accuracy of spamRF, using a threshold of 0.5 for predictions?
```{r}
cmat_RF=table(test$spam, predTestRF > 0.5)
cmat_RF
```
>(1291+387)/(1291+17+23+387) = 0.9767171

###### Problem 4.7 - Evaluating on the Test Set 
What is the testing set AUC of spamRF?
```{r}
predictionTestRF = prediction(predTestRF, test$spam)
pred = performance(predictionTestRF, "tpr", "fpr")
as.numeric(performance(predictionTestRF, "auc")@y.values)
```
>0.9975899
Both CART and random forest had very similar accuracies on the training and testing sets. However, logistic regression obtained nearly perfect accuracy and AUC on the training set and had far-from-perfect performance on the testing set. This is an indicator of overfitting.






